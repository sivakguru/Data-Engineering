{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Spark setup codes\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (3.0.1)\nRequirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.9)\n"
     ]
    }
   ],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "\n",
    "# install spark (change the version number if needed)\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
    "\n",
    "# unzip the spark file to the current folder\n",
    "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
    "\n",
    "# set your spark folder to your system path environment. \n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
    "\n",
    "\n",
    "# install findspark using pip\n",
    "!pip install -q findspark\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use findspark to initate the findspark and find the spark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/content/spark-3.0.0-bin-hadoop3.2'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# to validate if the findspark as found the correct instance\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up a sparksession, create a app\n",
    "# create a config to run the spark UI on port 4050\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"Colab\")\\\n",
    "        .config('spark.ui.port', '4050')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1e1a8adc50>"
      ],
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://3d8b53faa061:4051\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Colab</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# start the spark\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "t=2020-12-09T11:46:37+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n",
      "NgrokTunnel: \"http://257587862047.ngrok.io\" -> \"http://localhost:4050\"\n"
     ]
    }
   ],
   "source": [
    "#for having the SPARK UI to be ported to port 4050\n",
    "from pyngrok import ngrok\n",
    "print(ngrok.connect(4050))"
   ]
  },
  {
   "source": [
    "# Spark Data frames\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2020-12-09 11:46:38--  https://raw.githubusercontent.com/codeforamerica/ohana-api/master/data/sample-csv/mail_addresses.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1588 (1.6K) [text/plain]\nSaving to: ‘mail_addresses.csv.1’\n\nmail_addresses.csv. 100%[===================>]   1.55K  --.-KB/s    in 0s      \n\n2020-12-09 11:46:38 (27.8 MB/s) - ‘mail_addresses.csv.1’ saved [1588/1588]\n\n"
     ]
    }
   ],
   "source": [
    "# Use !wget to download csv files from a URL\n",
    "!wget https://raw.githubusercontent.com/codeforamerica/ohana-api/master/data/sample-csv/mail_addresses.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data in to data frame\n",
    "df = spark.read.csv('mail_addresses.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- location_id: integer (nullable = true)\n",
      " |-- attention: string (nullable = true)\n",
      " |-- address_1: string (nullable = true)\n",
      " |-- address_2: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state_province: string (nullable = true)\n",
      " |-- postal_code: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n",
      "+-------+------------------+------------------+--------------------+--------------------+---------+---------+--------------+-----------------+-------+\n",
      "|summary|                id|       location_id|           attention|           address_1|address_2|     city|state_province|      postal_code|country|\n",
      "+-------+------------------+------------------+--------------------+--------------------+---------+---------+--------------+-----------------+-------+\n",
      "|  count|                21|                21|                  20|                  21|        2|       21|            21|               21|     21|\n",
      "|   mean|              11.0|11.047619047619047|                null|                null|     null|     null|          null|90160.94736842105|   null|\n",
      "| stddev|6.2048368229954285| 6.288689771933344|                null|                null|     null|     null|          null|16523.95375902539|   null|\n",
      "|    min|                 1|                 1|Adult Rehabilitat...|1044 Middlefield ...|2nd Floor|  Fairfax|            CA|            22031|     US|\n",
      "|    max|                21|                22|         The Foodies|      P.O. Box 61868|Suite 100|Sunnyvale|            VA|            94403|     US|\n",
      "+-------+------------------+------------------+--------------------+--------------------+---------+---------+--------------+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show schema\n",
    "df.printSchema()\n",
    "\n",
    "#show columns as a list\n",
    "df.columns\n",
    "\n",
    "#describe the measure columns\n",
    "df.describe().show()"
   ]
  },
  {
   "source": [
    "# ways to get data, slice and dice dataframes\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------+\n|               city|\n+-------------------+\n|       Redwood City|\n|          San Mateo|\n|          San Mateo|\n|          San Mateo|\n|          San Mateo|\n|         Menlo Park|\n|         Menlo Park|\n|         Menlo Park|\n|       Redwood City|\n|       Redwood City|\n|       Redwood City|\n|       Redwood City|\n|       Redwood City|\n|       Redwood City|\n|      San Francisco|\n|          Sunnyvale|\n|South San Francisco|\n|       Redwood City|\n|          San Mateo|\n|            Fairfax|\n+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#select a column as singular column\n",
    "df.select('city').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------+-------+\n|               city|country|\n+-------------------+-------+\n|       Redwood City|     US|\n|          San Mateo|     US|\n|          San Mateo|     US|\n|          San Mateo|     US|\n|          San Mateo|     US|\n|         Menlo Park|     US|\n|         Menlo Park|     US|\n|         Menlo Park|     US|\n|       Redwood City|     US|\n|       Redwood City|     US|\n|       Redwood City|     US|\n|       Redwood City|     US|\n|       Redwood City|     US|\n|       Redwood City|     US|\n|      San Francisco|     US|\n|          Sunnyvale|     US|\n|South San Francisco|     US|\n|       Redwood City|     US|\n|          San Mateo|     US|\n|            Fairfax|     US|\n+-------------------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#select multiple columns\n",
    "df.select(['city','country']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a temp view out of dataframe , kind of temp table\n",
    "df.createOrReplaceTempView('mail_address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_result = spark.sql(\"select * from mail_address where city = 'San Mateo'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+--------------+-------+\n|     city|state_province|country|\n+---------+--------------+-------+\n|San Mateo|            CA|     US|\n|San Mateo|            CA|     US|\n|San Mateo|            CA|     US|\n|San Mateo|            CA|     US|\n+---------+--------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#filter the data frame based on a condition\n",
    "##sql_result.filter(\"location_id > 5\").show()\n",
    "\n",
    "#filter the data frame based on a condition, and select particular columns{this is using condition as SQL}\n",
    "sql_result.filter(\"location_id > 1 and location_id <=5 \").select(['city','state_province','country']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+--------------+-------+\n|     city|state_province|country|\n+---------+--------------+-------+\n|San Mateo|            CA|     US|\n|San Mateo|            CA|     US|\n|San Mateo|            CA|     US|\n|San Mateo|            CA|     US|\n+---------+--------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#filtering based on data frame, and multiple conditions not like sql\n",
    "sql_result.filter((sql_result['location_id'] > 1) & (sql_result['location_id'] <= 5)).select(['city','state_province','country']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using collect() function to store the values as a list object\n",
    "collect_result = sql_result.filter((sql_result['location_id'] > 1) & (sql_result['location_id'] <= 5)).select(['city','state_province','country']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'San Mateo'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "#accesing the values in the collected list object\n",
    "collect_result[0].asDict()['city']"
   ]
  },
  {
   "source": [
    "# spark group by operations\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data as dataframe\n",
    "# use below code to laod sales data in to data frame\n",
    "# !wget https://raw.githubusercontent.com/plotly/datasets/master/sales_success.csv\n",
    "df = spark.read.csv('sales_success.csv', header=True, inferSchema=True)\n",
    "# rename the column and make it persistent\n",
    "df = df.withColumnRenamed('_c0','id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+----------+----------+\n|region|sum(calls)|sum(sales)|\n+------+----------+----------+\n| South|       381|       193|\n|  East|       407|       160|\n|  West|       321|       189|\n| North|       631|       382|\n+------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# group by followed by type of aggregation and pass in the measure values names as items\n",
    "# the below code will aggreagate the data for all the measure/integet/numeric columns available\n",
    "# df.groupby('region').sum().show()  \n",
    "df.groupby('region').sum('calls', 'sales').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .agg method, takes in a dictionary\n",
    "#df.agg({'sales':'sum'}).show()\n",
    "\n",
    "# to specify the group by and have .agg method on top of it\n",
    "# assign the group by to a valiable and then perform the .agg aggregation\n",
    "\n",
    "by_region = df.groupBy('region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+----------+---------+----------+\n|region|sum(calls)|count(id)|sum(sales)|\n+------+----------+---------+----------+\n| South|       381|       12|       193|\n|  East|       407|       12|       160|\n|  West|       321|        9|       189|\n| North|       631|       18|       382|\n+------+----------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# by passing in a dictionary in .agg method we can specify what kind of aggregation is required for what measure column\n",
    "by_region.agg({'sales':'sum','calls':'sum','id':'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}